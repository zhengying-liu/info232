{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"logo.jpg\", width=150, ALIGN=\"left\">\n",
    "<center>\n",
    "<h1>Mini Projets 2019 (Info 232)</h1>\n",
    "Isabelle Guyon <br>\n",
    "info232@chalearn.org <br>\n",
    "</center>\n",
    "<span style=\"color:red\"> <h1> 1. Visualisation </h1> </span>\n",
    "    \n",
    "<p> We are visualizing the famous <a href=\"https://archive.ics.uci.edu/ml/datasets/iris\"> Iris dataset </a>.\n",
    "    \n",
    "<br> <b>Save your notebook often with menu File + Save and Checkpoint.</b>\n",
    "<br> <b>Before you push your homework to your GitHub repo, use  Kernel + Restart and Run all.</b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Instructions </h2>\n",
    "    <p>\n",
    "        Ce TP vaut 5 points. Repondez correctement a au moins 5 questions. <br>\n",
    "    <ul>\n",
    "        <li> Pour creer une nouvelle cellule, allez dans le menu \"Insert\".</li>\n",
    "        <li> Pour transformer une cellule en commentaire texte, allez dans Cell + Cell Type + Markdown. </li>\n",
    "        <li> Pour executer une cellule: SHIFT+RETURN </li>\n",
    "    </ul>\n",
    "    </p>\n",
    "    <p> <b> Les cellules doivent TOUTES etre executees et dans l'ordre.</b> Pour plus d'information <a href\"https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/What%20is%20the%20Jupyter%20Notebook.html\"> CONSULTEZ la DOCUMENTATION. </a>\n",
    "            \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 0: \"Markdown\" cells\n",
    "\n",
    "Cette question n'est pas notee, mais elle va vous aider.\n",
    "<br>Creez une <b>nouvelle cellule</b> de type Markdown en dessous de celle-ci.\n",
    "Recopiez dedans le paragraphe sur k-fold cross validation que vous trouverez sur Wikipedia. Essayez de colorer la cellule en VERT! \n",
    "<br> Idee: regardez le code de la cellule du dessus en double-cliquant dessus. \n",
    "<br> C'est bien pratique les cellules \"markdown\" pour inserer du texte dans votre code (ou des images, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< body>  j'ai mis un espace pour ne pas que le code soit execute car sinon je ne peux pas upload le fichier cependant sous                jupyter notebook le code color bien le texte en vert.\n",
    "  \n",
    "  <p style=\"color:gre en\" ;\n",
    "\n",
    "« k-fold cross-validation » : on divise l'échantillon original en {\\displaystyle k} k échantillons, puis on sélectionne un des {\\displaystyle k} k échantillons comme ensemble de validation et les {\\displaystyle k-1} k-1 autres échantillons constitueront l'ensemble d'apprentissage. On calcule comme dans la première méthode le score de performance, puis on répète l'opération en sélectionnant un autre échantillon de validation parmi les {\\displaystyle k-1} k-1 échantillons qui n'ont pas encore été utilisés pour la validation du modèle. L'opération se répète ainsi {\\displaystyle k} k fois pour qu'en fin de compte chaque sous-échantillon ait été utilisé exactement une fois comme ensemble de validation. La moyenne des {\\displaystyle k} k erreurs quadratiques moyennes est enfin calculée pour estimer l'erreur de prédiction .\n",
    "< /body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: \"Code\" cells\n",
    "\n",
    "Maintenant vous allez executer la cellule ci-dessous apres avoir replace la reponse par 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = 'code/'                        \n",
    "from sys import path; path.append(code_dir)\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from checker import check\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "question = 1\n",
    "answer = 1  # Replace by 1\n",
    "score = 0\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: AutoML format\n",
    "\n",
    "Les donnees que vous aurez a analyser dans votre projet seront au <a href=\"https://github.com/codalab/chalab/wiki/Help:-Wizard-%E2%80%90-Challenge-%E2%80%90-Data\">format AutoML</a>:\n",
    ".\n",
    "En utilisant les moyens que vous voulez, remplacez les valeurs des variables de dimension des donnees par leur valeurs correctes dans la deuxieme cellule avant de l'executer.\n",
    "<br> Idee 1: inspectez les fichiers avec un editeur. \n",
    "<br> Idee 2: remplacez la commande ls par une autre commande Unix comme wc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'             \n",
    "data_name = 'iris'\n",
    "!ls $data_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number = 4      # put correct value\n",
    "training_sample_number =35 # put correct value\n",
    "validation_sample_number = 35 # put correct value\n",
    "test_sample_number = 35  # put correct value\n",
    "question = 2\n",
    "reponse = feature_number*(training_sample_number+validation_sample_number+test_sample_number)\n",
    "score += check(reponse, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Pandas\n",
    "\n",
    "En Anglais pour changer :-) \n",
    "\n",
    "This time we are going to do simple \"exploratory data analysis\". To simplify, we lump all the data together in one big data structure calle a \"pandas\" data frame. This will allow us to use the rich libraries \"pandas\" and \"seaborn\" to explore the data. \n",
    "\n",
    "In the next cell, replace the \"head\" function, which just shows the first few rows of the dataset, by a padas function providing descriptive statistics. To that end, you may want to check the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/frame.html\">Pandas DataFrame reference page</a>. Then, in the following cell, replace the variables with their correct values and execute it.\n",
    "\n",
    "<br> Idee: il y a une fonction dans la page <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/frame.html\">Pandas DataFrame reference page</a> qui engrendre plein de <b>descriptive statistics</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io import read_as_df\n",
    "data = read_as_df(data_dir  + '/' + data_name)                # The data are loaded as a Pandas Data Frame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_sepal_length = 1.218279  # Standard deviation of the sepal length, put correct value\n",
    "mean_sepal_width = 3.18   # Mean of the sepal width, put correct value\n",
    "min_petal_length = 1.2  # Minimum value of the petal length, put correct value\n",
    "max_petal_width = 1.8    # Maximum value of the petal width, put correct value\n",
    "question = 3\n",
    "reponse = std_sepal_length+mean_sepal_width+min_petal_length+max_petal_width\n",
    "score += check(reponse, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Histograms\n",
    "\n",
    "Un truc sympa de Pandas c'est que ca permet aussi de faire des graphes pour visualiser les donnees. Remplacez le nombre de bins par un plus petit nombre. Repondez en changeant la variable answer dans la deuxieme cellule: answer=1 si la hauteur de la plus grande barre augmente et answer=0 sinon. Comprenez-vous pourquoi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=(10, 10), bins=50, layout=(3, 2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 4\n",
    "answer = 1           # 1 if the maximum bar height increases when the bin number decreses, 0 otherwise\n",
    "score += check(answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have fun with the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/frame.html\">Pandas DataFrame reference page</a>. There are lots of other plotting functions. Try some of them in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's this one for instance? Change it to something else.\n",
    "data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Pair plots\n",
    "\n",
    "Seaborn (sns) is a package of data visualization functions: https://seaborn.pydata.org/. Quite useful! It is convenient to visualize data in 2 dimensions. One way of doing that is to plot a variable (feature) against another one, one point representing a sample (a flower). The pairplot function shows all the possibilities (off-diagonal graphs). \n",
    "\n",
    "On the diagonal, what do you see? Compare with the histograms of the previous question.\n",
    "Then add another argument to the pairplot function \n",
    "        \n",
    "        hue=\"target\" \n",
    "        \n",
    "(if you do not understand, consult the documentation https://seaborn.pydata.org/generated/seaborn.pairplot.html). After executing the next cell again, in the folowing cell, answer the questions: \n",
    "\n",
    "What is the color of the class, which is best separated from all others?\n",
    "\n",
    "        color_best_separated = 1 if blue; 2 if orange; 3 if green.\n",
    "\n",
    "Which iris type does this correspond to?\n",
    "\n",
    "        iris_best_separated = 1 if virginica; 2 if versocolor; 3 if setosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "sns.pairplot(data), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 5\n",
    "color_best_separated = 1 # Change that\n",
    "iris_best_separated = 3  # Change that\n",
    "score += check(color_best_separated*iris_best_separated, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Feature correlation\n",
    "Le variables (features) peuvent etre redondantes (c'est a dire capturer des informations similaires). Le coefficient de correlation de Pearson (voir <a href =\"https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\">page Wikipedia</a>) permet de detecter la correlation (c'est a dire la similarite au sens d'une dependance lineaire). \n",
    "\n",
    "En regardant les \"pair plots\" ci-dessus, a votre avis, quelle paire de variable est la plus correlee? Reperez la paire par son numero de ligne et de colonne pour repondre. Verifiez votre intuition en executant la cellule suivante qui represente graphiquement la matrice de correlation. Changez la methode 'pearson' pour d'autres coefficients de correlation (voir <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\">documentation</a>). Est-ce que ca change quelle paire est la plus correlee? Regardez les definitions de <a href =\"Kandall tau\">https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient</a> et <a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\">Spearman correlation coefficient</a>. Essayez de comprendre la difference entre <a href=\"https://en.wikipedia.org/wiki/Correlation_and_dependence\">correlation et dependence</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 6\n",
    "numero_ligne = 3             # Lignes numerotee de 0 a 3, remplacer la reponse\n",
    "numero_colonne = 2           # Colonnes numerotee de 0 a 3, remplacer la reponse\n",
    "score += check(numero_ligne*numero_colonne, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = data.corr(method='pearson')\n",
    "sns.heatmap(corr_mat, annot=True, center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarquez que les variables peuvent etre correlee ou anti-corelees. Vous voulez donc peut-etre vous interesser a la valeur absolue du coefficient de correlation. Cela change-t-il votre reponse? Peut-etre pas, mais ca pourrait!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(abs(corr_mat), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que la matrice est symetrique. Si vous voulez vous amuser, effacez les valeurs au dessus de la diagonale (<a href=\"https://seaborn.pydata.org/generated/seaborn.heatmap.html\">voir en bas de cette page</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Feature selection\n",
    "Representing a matrix of coefficients with colors seems to be pretty convenient for visualization purposes. We would like to do that also for the data matrix itself. Note that, since the last column (target) contains strings (\"categorical variables\"), we first need to convert them to numbers. \n",
    "\n",
    "Observing the heatmap, which column is most correlated with the target? Insert another cell in which you plot the correlation matrix of data_new (inspiring yourself from the previous question), then confirm your intuition and anwer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())\n",
    "data_num = data.copy()  # If you don't use \"copy\", any change in data_num will also result in a change in data\n",
    "data_num['target']= data_num['target'].astype('category')\n",
    "data_num['target'] = data_num['target'].cat.codes\n",
    "print(data_num.head())\n",
    "sns.heatmap(data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettez ici votre code montrant la matrice de correlation de data_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La question 7 est ici:\n",
    "Quelle est la variable (feature) la plus correlee avec la colonne \"target\"? Quelle est la valeur du coefficient de correlation de Pearson correspondant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 7\n",
    "numero_variable = 0            # Variables numerotees de 0 a 3, replacez la reponse\n",
    "pearson_correlation = 0        # Mettre la valeur correcte\n",
    "score += check(numero_variable+pearson_correlation, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: One-Rule classifier\n",
    "In this section, we show how we can create a very simple classifier based on just ONE rule to separate the 3 types of flowers. That rule classifies irises on the basis of their petal length only. Check the code to see whether you understand it.\n",
    "\n",
    "This classifier respects the structure of <a href =\"https://scikit-learn.org/stable/\">scikit-learn</a> learning machines. To make it compatible with other scikit-learn tools, we derive it from the base class BaseEstimator and overload 2 methods: \"fit\" and \"predict\". Then we use the Iris data to train and test a model (in this case we lumped all the data into a single matrix and use it as training data). After that we compute the training error.\n",
    "\n",
    "Scikit-learn allows you to compute <a href =\"https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics\"> a lot of other metrics </a>. To answer this question, you will have to compute the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score\">BAC</a>, that is the Balanced ACcuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class oneR(BaseEstimator):\n",
    "    ''' One Rule classifier '''\n",
    "    def __init__(self):\n",
    "        ''' The \"constructor\" initializes the parameters '''\n",
    "        self.selected_feat = 0 \t# The chosen variable/feature\n",
    "        self.theta1 = 0 \t\t# The first threshold\n",
    "        self.theta2 = 0\t\t\t# The second threshold\n",
    "\n",
    "    def fit(self, X, Y, F=[]):\n",
    "        ''' The method \"fit\" trains a super-simple classifier '''\n",
    "        if not F: F=[str(item) for item in range(X.shape[1])]\n",
    "        # First it selects the feature most correlated to the target\n",
    "        correlations = np.corrcoef(X, Y, rowvar=0)\n",
    "        self.selected_feat = np.argmax(correlations[0:-1, -1])\n",
    "        best_feat = X[:, self.selected_feat]\n",
    "        print('Feature selected = ' +  F[self.selected_feat])\n",
    "        # Then it computes the average values of the 3 classes\n",
    "        mu0 = np.median(best_feat[Y==0])\n",
    "        mu1 = np.median(best_feat[Y==1])\n",
    "        mu2 = np.median(best_feat[Y==2])\n",
    "        # Finally is sets two decision thresholds\n",
    "        self.theta1 = (mu0+mu1)/2.\n",
    "        self.theta2 = (mu1+mu2)/2.\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' The method \"predict\" classifies new test examples '''\n",
    "        # Select the values of the correct feature\n",
    "        best_feat = X[:, self.selected_feat]\n",
    "        # Initialize an array to hold the predicted values\n",
    "        Yhat = np.copy(best_feat)\t\t\t\t# By copying best_fit we get an array of same dim\n",
    "        # then classify using the selected feature according to the cutoff thresholds\n",
    "        Yhat[best_feat<self.theta1] = 0\t\t\t\t\t\t\t\t\t\t\t# Class 0\n",
    "        Yhat[np.all([self.theta1<=best_feat, best_feat<=self.theta2], 0)] = 1\t# Class 1\n",
    "        Yhat[best_feat>self.theta2] = 2 \t\t\t\t\t\t\t\t\t\t# Class 2\n",
    "        return Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY=data_num.as_matrix()\t\t\t\t# On transforme le Pandas dataframe en un Numpy array\n",
    "X = XY[:,0:4]\t\t\t\t\t\t# On recupere X (num_exemples x num_features)\n",
    "Y = XY[:,4]\t\t\t\t\t\t\t# et Y (num_exemples x 1) ==> les valeurs de target\n",
    "feature_names = list(data_num)[:-1] # On recupere aussi les noms des features\n",
    "my_model = oneR()\n",
    "my_model.fit(X, Y, feature_names)\t# Le nom des features est \"optionnel\", il peut etre supprime\n",
    "Yhat = my_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(solution, prediction):\n",
    "    return np.mean(solution!=prediction)\n",
    "    \n",
    "errate = error_rate(Y, Yhat)\n",
    "print('Training error = %5.2f' % errate)\n",
    "Yperm = np.random.permutation(Y)\n",
    "print('Random permutation error (for comparison)= %5.2f' % error_rate(Y, Yperm))\n",
    "print('Ideal error rate (for comparison)= %5.2f' % error_rate(Y, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La question 8 est ici:\n",
    "Pour les problemes multiclasses dont les classes ne sont pas bien equilibree (une classe a beaucoup moins d'exemples qu'une autre), le \"balanced error rate\" est une meilleure metrique de l'\"error rate\". On obtient le \"balanced error rate\" a partir de la matrice de confusion. La version la plus recente de scikit-learn a une implementation du <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\">\"balanced accuracy\"</a>, c'est a dire (1-balanced_error_rate).\n",
    "<br> 1) En vous basant sur le code [<a href =\"https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/metrics/classification.py#L1371\">source</a>] de <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\">\"balanced accuracy\"</a>, programmez une fonction qui calcule le balanced_error_rate.\n",
    "<br> 2) Observez que le resultat de error_rate est different de celui de balanced_error_rate.\n",
    "<br> 3) Verifiez votre comprehension en calculant \"a la main\" le balanced_error_rate a partir de la matrice de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(Y, Yhat))\n",
    "\n",
    "def balanced_error_rate(solution, prediction):\n",
    "    # Put here the code of balanced_accuracy and return 1-balanced_accuracy\n",
    "    return 0\n",
    "\n",
    "BER = balanced_error_rate(Y, Yhat)\n",
    "print('BER = %5.2f' % BER)\n",
    "question = 8\n",
    "score += check(BER, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Viewing the classification results\n",
    "Pour se faire une idee de comment marche les classifieurs il est utile de visualiser les regions de decision. Ce petit programme permet de le faire en deux dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = [(1, 0, 0), (0, 1, 0), (0, 0, 1)] # Red, lime, blue\n",
    "cm = LinearSegmentedColormap.from_list('rgb', colors, N=3)\n",
    "\n",
    "def ClfScatter(clf, X, Y, F, dim1=0, dim2=1, title=''):\n",
    "    '''clf_scatter(clf, X, Y, F, dim1=0, dim2=1)\n",
    "    Display decision function and training examples.\n",
    "    clf: a classifier with at least a fit and a predict method\n",
    "    like a sckit-learn classifier.\n",
    "    X: a 2 dimensional data matrix, samples in line and features in columns\n",
    "    Y: a target vectors of class values 0, 1, 2, etc.\n",
    "    F: feature names\n",
    "    dim1 and dim2: chosen features.\n",
    "    title: Figure title.\n",
    "    Returns: Predictions on training examples.\n",
    "    '''\n",
    "    # Fit model in chosen dimensions\n",
    "    X2 = X[:,(dim1,dim2)]\n",
    "    try:\n",
    "        clf.fit(X2, Y, F=[F[dim1],F[dim2]])\n",
    "    except:\n",
    "        clf.fit(X2, Y)\n",
    "    # Define a mesh    \n",
    "    x_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\n",
    "    y_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\n",
    "    h = 0.1 # step\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Xtest = np.c_[xx.ravel(), yy.ravel()]\n",
    "    # Compute the training error\n",
    "    Yhat = clf.predict(X2) \n",
    "    training_error = error_rate(Y, Yhat)\n",
    "    # Make your predictions on all mesh grid points (test points)\n",
    "    Yhat = clf.predict(Xtest) \n",
    "    # Make contour plot for all points in mesh\n",
    "    Yhat = Yhat.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Yhat, cmap=plt.cm.Paired)\n",
    "    # Overlay scatter plot of training examples\n",
    "    plt.scatter(X2[:, 0], X2[:, 1], c=Y, cmap=cm)   \n",
    "    plt.title('{}: training error = {:5.2f}'.format(title, training_error))\n",
    "    plt.xlabel(F[dim1])\n",
    "    plt.ylabel(F[dim2])\n",
    "    plt.show()\n",
    "    return clf.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = ClfScatter(my_model, X, Y, feature_names, dim1=0, dim2=1, title='OneR')\n",
    "errate = error_rate(Y, Yhat)\n",
    "print('Training error = %5.2f' % errate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La question 9 est ici:\n",
    "Replacez dim1=2, dim2=3 par dim1=0, dim2=1 dans la cellule du dessus et re-executez la. Vous devriez obtenir de moins bonnes performances. Pensez vous qu'un classifieur qui utiliserait plusieurs regles pourrait obtenir 0 erreurs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 9\n",
    "reponse = 1 # Oui = 1, Non = 0\n",
    "score += check(errate+reponse, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10: More classifiers\n",
    "En utilisant le code precedant, on peut comparer plein de classifieurs. Certains n'obtiennent pas les memes resultats si on les fait tourner plusieurs fois, d'autres donnent toujours la meme chose. Combien d'entre eux (parmi les exemples ci-dessous) donnent toujours la meme chose? Verifiez bien dans la doc de <a href =\"https://scikit-learn.org/stable/\">scikit-learn</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "classifier_dict = {'Perceptron': Perceptron(random_state=random.randint(1,101)),\n",
    "                   'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),\n",
    "                   'Gaussian Classifier': GaussianNB(),\n",
    "                   'Decision Tree': DecisionTreeClassifier(random_state=random.randint(1,101)),\n",
    "                   'Support Vector Machine': SVC()}\n",
    "for key in classifier_dict:\n",
    "    # This does a two dimensional fit in dim1 and dim2\n",
    "    clf = classifier_dict[key]\n",
    "    ClfScatter(clf, X, Y, feature_names, dim1=2, dim2=3, title=key)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 10\n",
    "reponse = 3 # Nombre de classifieurs qui retournent toujours la meme separation des classes; changer\n",
    "score += check(reponse, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Your final score is %d / 10, congratulations!' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
